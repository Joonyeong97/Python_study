RNN
Recurrent neural network
순서를 추출
Cell 	+	 Cell
(가중치)
- input data도 vector로 들어온다
cell에 가중치가 있어서 데이터가 가중치와 만나서 나오면 특성이 된다.
그 특성이 다음셀에 전달이 된다.
그 특성과 input data를 같이 cell에 들어가기 때문에 시간이 걸린다.
기울기 소실 문제가 생긴다
계속 셀에 넘겨지면서 영향력이 누적되어서 과도한 영향력을 받게되면서 기울기가 소실된다.
이런 문제를 해결하기 위해서 나온것이 LSTM 이다
회로를 두개를 만들어서 State가 두개다.
seq2seq -> 전에생긴 특성이 들어와야 하기 때문에 처음 입력 데이터는 빈공간으로 만들기
GRU (계산이 작다. 빈공간을 만들어야 하기 때문에)

Time Series 와 Text mining을 다루는것이 목적 
Time Data Generator이 해주는일
시계열 데이터는 자기 상관성을 따짐.
window( 그전 시간들이 그 다음 시간에 영향을 주는것을 보는것)
(10개를 보고 그 다음시간과 영향을 보고, 또 그다음 10개를 묶어서 다시보는 방식) 
즉 주식데이터처럼 시계열데이터
정상서을 띈 데이터만 사용가능 (분산 , 공산성 등이 일정)
ex) AR , MA (auto regressiong 자기회귀 ,moving average 이동평균법 ( ARMA)
비 정상성을 '차분'해서 정상성을 만든다
차분: 뒤에 데이터 에서 앞의 데이터를 뺀다 == 미분한다
비선형적으로 시계열 분석을 해주는것이 RNN
2. Text mining
I Have a pen -> token mization(단어 하나하나를 잘라서 DICT처리)
I : 0, Have :1, a:2, pen:3 이런식으로
단어에 번호를 매기고 이런식으로 나눠진게 ONEHOT ENCODING

vectorization (신경망에서 가중치를 만나 사이즈를 줄여주고 근처에 가까운 단어를 확인한다.)
 - 가까운 단어는 비슷한 벡터를 준다
word2vec /vector를 가지고 단어 또는 문장을 찾을 수 있게 한다.
word2vec : 벡터로 훈련시켜서 같이 사용되는 단어를 표현
- layer를 지원한다.
- 결국 숫자화 되어야 하고 그게 벡터화 되어져서 특징을 추천(가까이에 있는 단어의 유사도)
- attention : 데이터가 들어가면 가중치에서 query / key /value를 가진다.
	-attention point ( 가중치 + key )
	-attention value (가중치 + value)
NMT : attention 망을 가져서 단어들의 중요도를 뽑는다.

LSTM과 NMT의 중요도를 같이 사용해서 단어의 특징을 더 장 추출한다.
Transformer: attention망과 LSTM망을 여러개 쌓아서 만든것
BEST는 트랜스포머를 만들어진것

seq2seq로 할수 있는것
1. translation
2. chatbot
3. summerization

- LSTM & BERT : seq2seq의 발전 버전