{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lines= pd.read_csv('dataset.txt', names=['src', 'tar'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bb9d2d4a7deb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtar_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mtar_vocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "src_vocab=set()\n",
    "for line in lines.src: # 1줄씩 읽음\n",
    "    for char in line: # 1개의 글자씩 읽음\n",
    "        src_vocab.add(char)\n",
    "\n",
    "tar_vocab=set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"float\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-de518abffe5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m'\\t '\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' \\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_test\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4043\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4044\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4045\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4047\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-de518abffe5b>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m'\\t '\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' \\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"float\") to str"
     ]
    }
   ],
   "source": [
    "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"float\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-67f125edda23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#lines = get_data(data_set['jpn']).split('\\n')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m'\\t '\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' \\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_test\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4043\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4044\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4045\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4047\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-67f125edda23>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#lines = get_data(data_set['jpn']).split('\\n')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m'\\t '\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' \\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"float\") to str"
     ]
    }
   ],
   "source": [
    "# 문장 벡터화\n",
    "batch_size = 100\n",
    "epochs = 24\n",
    "latent_dim = 256\n",
    "num_samples = 4096\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "\n",
    "\n",
    "#lines = get_data(data_set['jpn']).split('\\n')\n",
    "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "lines.sample(10)\n",
    " \n",
    "  # 문자 집합 생성\n",
    "for char in input_text:\n",
    "    if char not in input_characters:\n",
    "        input_characters.add(char)\n",
    "for char in target_text:\n",
    "    if char not in target_characters:\n",
    "        target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "from keras import datasets\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "import matplotlib\n",
    "from matplotlib import ticker\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#from IPython.display import Image, display\n",
    "  \n",
    "def get_data(file_id):\n",
    "  \"\"\"\n",
    "  구글 드라이브에서 file_id에 해당하는 파일을 가져와 읽습니다.\n",
    "  Colab에서 돌리기 위해 필요합니다.\n",
    "  \"\"\"\n",
    "  \n",
    "  # Install the PyDrive wrapper & import libraries.\n",
    "  # This only needs to be done once per notebook.\n",
    "    from pydrive.auth import GoogleAuth\n",
    "    from pydrive.drive import GoogleDrive\n",
    "    from google.colab import auth\n",
    "    from oauth2client.client import GoogleCredentials\n",
    " \n",
    "    # Authenticate and create the PyDrive client.\n",
    "    # This only needs to be done once per notebook.\n",
    "    auth.authenticate_user()\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.credentials = GoogleCredentials.get_application_default()\n",
    "    drive = GoogleDrive(gauth)\n",
    "\n",
    "    # Download a file based on its file ID.\n",
    "    #\n",
    "    # A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
    "    downloaded = drive.CreateFile({'id': file_id})\n",
    "    return downloaded.GetContentString()\n",
    "\n",
    "# 학습 정보\n",
    "batch_size = 100\n",
    "epochs = 24\n",
    "latent_dim = 256\n",
    "num_samples = 4096\n",
    " \n",
    "# 문장 벡터화\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    " \n",
    "data_set = {\n",
    "    'kor': '12UL7MH38rxRFskhYuj1B9zU7wUXgv8Q7',\n",
    "    'jpn': '1ULdUnurb_DEDTzJFaycdWHWkYWqPu4Bo',\n",
    "    'deu': '1OhP8vPwTGrLyweo0HuLhhGpe0VRWn4k6',\n",
    "}\n",
    " \n",
    "lines = get_data(data_set['jpn']).split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    target_text = input_text # 임시\n",
    "\n",
    "    # \"\\t\"문자를 시작 문자로, \"\\n\"문자를 종료 문자로 사용.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    " \n",
    "  # 문자 집합 생성\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "  # 학습 데이터 개수\n",
    "num_samples = len(input_texts)\n",
    "            \n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    " \n",
    "print('Number of samples:', num_samples)\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "# 문자 -> 숫자 변환용 사전\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "# 학습에 사용할 데이터를 담을 3차원 배열\n",
    "encoder_input_data = np.zeros(\n",
    "    (num_samples, max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (num_samples, max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (num_samples, max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# 문장을 문자 단위로 원 핫 인코딩하면서 학습용 데이터를 만듬\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "          decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "# 숫자 -> 문자 변환용 사전\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "def RepeatVectorLayer(rep, axis):\n",
    "    return layers.Lambda(lambda x: K.repeat_elements(K.expand_dims(x, axis), rep, axis),\n",
    "                      lambda x: tuple((x[0],) + x[1:axis] + (rep,) + x[axis:]))\n",
    "\n",
    "# 인코더 생성\n",
    "encoder_inputs = layers.Input(shape=(max_encoder_seq_length, num_encoder_tokens))\n",
    "encoder = layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h = encoder(encoder_inputs)\n",
    "\n",
    "# 디코더 생성.\n",
    "decoder_inputs = layers.Input(shape=(max_decoder_seq_length, num_decoder_tokens))\n",
    "decoder = layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _ = decoder(decoder_inputs, initial_state=state_h)\n",
    "\n",
    "# 어텐션 매커니즘.\n",
    "repeat_d_layer = RepeatVectorLayer(max_encoder_seq_length, 2)\n",
    "repeat_d = repeat_d_layer(decoder_outputs)\n",
    "\n",
    "repeat_e_layer = RepeatVectorLayer(max_decoder_seq_length, 1)\n",
    "repeat_e = repeat_e_layer(encoder_outputs)\n",
    "\n",
    "concat_for_score_layer = layers.Concatenate(axis=-1)\n",
    "concat_for_score = concat_for_score_layer([repeat_d, repeat_e])\n",
    "\n",
    "dense1_t_score_layer = layers.Dense(latent_dim // 2, activation='tanh')\n",
    "dense1_score_layer = layers.TimeDistributed(dense1_t_score_layer)\n",
    "dense1_score = dense1_score_layer(concat_for_score)\n",
    "dense2_t_score_layer = layers.Dense(1)\n",
    "dense2_score_layer = layers.TimeDistributed(dense2_t_score_layer)\n",
    "dense2_score = dense2_score_layer(dense1_score)\n",
    "dense2_score = layers.Reshape((max_decoder_seq_length, max_encoder_seq_length))(dense2_score)\n",
    "\n",
    "softmax_score_layer = layers.Softmax(axis=-1)\n",
    "softmax_score = softmax_score_layer(dense2_score)\n",
    "\n",
    "repeat_score_layer = RepeatVectorLayer(latent_dim, 2)\n",
    "repeat_score = repeat_score_layer(softmax_score)\n",
    "\n",
    "permute_e = layers.Permute((2, 1))(encoder_outputs)\n",
    "repeat_e_layer = RepeatVectorLayer(max_decoder_seq_length, 1)\n",
    "repeat_e = repeat_e_layer(permute_e)\n",
    "\n",
    "attended_mat_layer = layers.Multiply()\n",
    "attended_mat = attended_mat_layer([repeat_score, repeat_e])\n",
    "\n",
    "context_layer = layers.Lambda(lambda x: K.sum(x, axis=-1),\n",
    "                             lambda x: tuple(x[:-1]))\n",
    "context = context_layer(attended_mat)\n",
    "\n",
    "concat_context_layer = layers.Concatenate(axis=-1)\n",
    "concat_context = concat_context_layer([context, decoder_outputs])\n",
    "\n",
    "attention_dense_output_layer = layers.Dense(latent_dim, activation='tanh')\n",
    "attention_output_layer = layers.TimeDistributed(attention_dense_output_layer)\n",
    "attention_output = attention_output_layer(concat_context)\n",
    "\n",
    "decoder_dense = layers.Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(attention_output)\n",
    "\n",
    "# 모델 생성\n",
    "model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "choice = input(\"Load weights?\")\n",
    "if choice == 'y' or choice == 'Y':\n",
    "    model.load_weights('att_seq2seq_weights.h5')\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "             metrics=['acc'])\n",
    "#model.summary()\n",
    "#plot_model(model, show_shapes=True, to_file='model.png')\n",
    "#display(Image(filename='model.png'))\n",
    "\n",
    "choice = input(\"Train?\")\n",
    "if choice == 'y' or choice == 'Y':\n",
    "  # 학습\n",
    "  history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      validation_split=0.2,\n",
    "                      verbose=2)\n",
    "\n",
    "    model.save_weights('att_seq2seq_weights.h5')\n",
    "\n",
    "  # 손실 그래프\n",
    "    plt.plot(history.history['loss'], 'y', label='train loss')\n",
    "    plt.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # 정확도 그래프\n",
    "    plt.plot(history.history['acc'], 'y', label='train acc')\n",
    "    plt.plot(history.history['val_acc'], 'r', label='val acc')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "# 어텐션 검증\n",
    "test_data_num = 0\n",
    "test_max_len = 0\n",
    "for i, s in enumerate(input_texts):\n",
    "    if len(s) > test_max_len:\n",
    "        test_max_len = len(s)\n",
    "        test_data_num = i\n",
    "\n",
    "test_enc_input = encoder_input_data[test_data_num].reshape(\n",
    "    (1, max_encoder_seq_length, num_encoder_tokens))\n",
    "test_dec_input = decoder_input_data[test_data_num].reshape(\n",
    "    (1, max_decoder_seq_length, num_decoder_tokens))\n",
    "\n",
    "attention_layer = softmax_score_layer\n",
    "func = K.function([encoder_inputs, decoder_inputs] + [K.learning_phase()], [attention_layer.output])\n",
    "score_values = func([test_enc_input, test_dec_input, 1.0])[0]\n",
    "score_values = score_values.reshape((max_decoder_seq_length, max_encoder_seq_length))\n",
    "\n",
    "score_values = score_values[:len(target_texts[test_data_num])-1, :len(input_texts[test_data_num])]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(score_values, interpolation='nearest')\n",
    "fig.colorbar(cax)\n",
    "\n",
    "test_enc_names = []\n",
    "for vec in test_enc_input[0]:\n",
    "    sampled_token_index = np.argmax(vec)\n",
    "    sampled_char = reverse_input_char_index[sampled_token_index]\n",
    "    test_enc_names.append(sampled_char)\n",
    "test_dec_names = []\n",
    "for vec in test_dec_input[0]:\n",
    "    sampled_token_index = np.argmax(vec)\n",
    "    sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "    test_dec_names.append(sampled_char)\n",
    "\n",
    "print(test_dec_names[1:len(target_texts[test_data_num])])\n",
    "\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.set_yticklabels(['']+test_dec_names[1:-1] + ['<END>'])\n",
    "ax.set_xticklabels(['']+test_enc_names)\n",
    "\n",
    "plt.show()\n",
    "  \n",
    "# 추론(테스트)\n",
    "\n",
    "# 추론 모델 생성\n",
    "encoder_model = models.Model(encoder_inputs, [encoder_outputs, state_h])\n",
    "encoder_outputs_input = layers.Input(shape=(max_encoder_seq_length, latent_dim))\n",
    "\n",
    "decoder_inputs = layers.Input(shape=(1, num_decoder_tokens))\n",
    "decoder_state_input_h = layers.Input(shape=(latent_dim,))\n",
    "decoder_outputs, decoder_h = decoder(decoder_inputs, initial_state=decoder_state_input_h)\n",
    "\n",
    "repeat_d_layer = RepeatVectorLayer(max_encoder_seq_length, 2)\n",
    "repeat_d = repeat_d_layer(decoder_outputs)\n",
    "\n",
    "repeat_e_layer = RepeatVectorLayer(1, axis=1)\n",
    "repeat_e = repeat_e_layer(encoder_outputs_input)\n",
    "\n",
    "concat_for_score_layer = layers.Concatenate(axis=-1)\n",
    "concat_for_score = concat_for_score_layer([repeat_d, repeat_e])\n",
    "\n",
    "dense1_score_layer = layers.TimeDistributed(dense1_t_score_layer)\n",
    "dense1_score = dense1_score_layer(concat_for_score)\n",
    "\n",
    "dense2_score_layer = layers.TimeDistributed(dense2_t_score_layer)\n",
    "dense2_score = dense2_score_layer(dense1_score)\n",
    "dense2_score = layers.Reshape((1, max_encoder_seq_length))(dense2_score)\n",
    "\n",
    "softmax_score_layer = layers.Softmax(axis=-1)\n",
    "softmax_score = softmax_score_layer(dense2_score)\n",
    "\n",
    "repeat_score_layer = RepeatVectorLayer(latent_dim, 2)\n",
    "repeat_score = repeat_score_layer(softmax_score)\n",
    "\n",
    "permute_e = layers.Permute((2, 1))(encoder_outputs_input)\n",
    "repeat_e_layer = RepeatVectorLayer(1, axis=1)\n",
    "repeat_e = repeat_e_layer(permute_e)\n",
    "\n",
    "attended_mat_layer = layers.Multiply()\n",
    "attended_mat = attended_mat_layer([repeat_score, repeat_e])\n",
    "\n",
    "context_layer = layers.Lambda(lambda x: K.sum(x, axis=-1),\n",
    "                             lambda x: tuple(x[:-1]))\n",
    "context = context_layer(attended_mat)\n",
    "\n",
    "concat_context_layer = layers.Concatenate(axis=-1)\n",
    "concat_context = concat_context_layer([context, decoder_outputs])\n",
    "\n",
    "attention_output_layer = layers.TimeDistributed(attention_dense_output_layer)\n",
    "attention_output = attention_output_layer(concat_context)\n",
    "\n",
    "decoder_att_outputs = decoder_dense(attention_output)\n",
    "\n",
    "decoder_model = models.Model([decoder_inputs, decoder_state_input_h, encoder_outputs_input],\n",
    "                            [decoder_outputs, decoder_h, decoder_att_outputs])\n",
    "#decoder_model.summary()\n",
    "#plot_model(decoder_model, show_shapes=True, to_file='decoder_model.png')\n",
    "#display(Image(filename='decoder_model.png'))\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "  # 입력 문장을 인코딩\n",
    "    enc_outputs, states_value = encoder_model.predict(input_seq)\n",
    " \n",
    "  # 디코더의 입력으로 쓸 단일 문자\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # 첫 입력은 시작 문자인 '\\t'로 설정\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    " \n",
    "  # 문장 생성\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        # 이전의 출력, 상태를 디코더에 넣어서 새로운 출력, 상태를 얻음\n",
    "        # 이전 문자와 상태로 다음 문자와 상태를 얻는다고 보면 됨.\n",
    "        dec_outputs, h, output_tokens = decoder_model.predict(\n",
    "            [target_seq, states_value, enc_outputs])\n",
    "\n",
    "        # 사전을 사용해서 원 핫 인코딩 출력을 실제 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # 종료 문자가 나왔거나 문장 길이가 한계를 넘으면 종료\n",
    "        if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    " \n",
    "        # 디코더의 다음 입력으로 쓸 데이터 갱신\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = h\n",
    " \n",
    "  return decoded_sentence\n",
    "\n",
    "for seq_index in range(30):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('\"{}\" -> \"{}\"'.format(input_texts[seq_index], decoded_sentence.strip()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
