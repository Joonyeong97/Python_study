{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youtube CNN\n",
    "# https://www.youtube.com/watch?v=UmAa2wyyHkw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "입력데이터 * 필터링 -> 컨볼루션 연산결과 + 바이어스 -> \n",
    "특징맵 -> Relu-> pooling\n",
    "-> 완전연결층( FLATTEN( 평평하게 ) ) -> Linear regression -> \n",
    "softmax  -> 손실함수 최소값 확인\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution 이란? \n",
    "입력데이터와 가중치들의 집합체인 다양한 필터와의\n",
    "컨볼루션 연산을 통해서 입력데이터의 특징을 추출하는 역할\n",
    "\n",
    "pooling 이란?\n",
    "입력정보를 최대값 or 최소값 or 평균값등으로 압축하여 데이터\n",
    "연산량을 줄여주는 역할 수행\n",
    "일반적으로 max값으로 출력을 진행함\n",
    "\n",
    "padding 이란?\n",
    "패딩이란 컨볼루션 연산을 수행하기 전에 입력데이터 주변을 특정 값\n",
    "으로 채우는것을 말하며, 컨볼루션 연산에서 자주 이용되는방법\n",
    "컨볼루션 연산을 수행하면 데이터 크기가(shape) 줄어드는 단점을\n",
    "방지하기 위함\n",
    "\n",
    "softmax란?\n",
    "입력받은 값을 출력으로 0~1사이의 값으로 모두 정규화하며 출력값들의\n",
    "총합은 항상 1이 되도록 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력층 -> 컨볼루션층 -> 완전연결층 -> 출력층 -> 손실함수 최소값확인 -> 학습종료\n",
    "\n",
    "이미지입력하면\n",
    "conv층에서 입력층*필터+바이어스 를 스프라이드 횟수만큼 옆으로 필터링을 진행함.\n",
    "그 후 필터링된 값을 relu로 진행해주면 마이너스인값들을 다 0으로 변환해주고 나머지는 그대로 진행\n",
    "그 후 pooling을 진행 예를들어 2*2 max pooling이면\n",
    "2*2 행렬에서 제일 큰값을 저장해서 하나의 행렬을 만들어서\n",
    "다음 컨볼루션층으로 보냄\n",
    "\n",
    "padding은 입력층에서 필터링되기전에 입히고서 진행해야함.\n",
    "필터링되고나서 넣는게 아님.\n",
    "\n",
    "마지막 컨볼루션 층에서 완전연결층으로 전달되면\n",
    "FLATTEN(평탄화) 작업으로 1차원 벡터로 reshape 진행\n",
    "그 후 Linear Regression을 한뒤\n",
    "softmax를 진행한뒤\n",
    "\n",
    "손실함수 최소값이 되면 학습종료 아니면 다시 재생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- conv = tf.nn.conv2d(...)\n",
    "- relu = tf.nn.relu(...)\n",
    "- pooling = tf.nn.max_pool(...)\n",
    "- FLATTEN = tf.reshape(conv마지막값,...)\n",
    "- softmax = tf.nn.softmax(linear값)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.conv2d(input, fillter, strides, padding, ...)\n",
    "밑에 파라미터 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input : 컨볼루션 연산을 위한 입력 데이터이며 \n",
    "    [batch, in_height,inwidth,in_channels]\n",
    "예를들어 100개의 배치로 묶은 28*28 크기의 흑백 이미지를 입력으로\n",
    "넣을경우 input은 [100,28,28,1]로 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillter : 컨볼루션 연산에 적용할 필터이며 \n",
    "    [fillter_height,fillter_width,in_channels,out_channels]\n",
    "예를들어, 필터 크기 3*3이며 입력채널 개수는 1이고 적용되는 필터 개수가\n",
    "총 32개이면 fillter는 [3,3,1,32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strides : 컨볼루션 연산을 위해 필터를 이동시키는 간격을 나타냄,\n",
    "    예를들어 [1, 1, 1, 1]로 strides를 나타낸다면\n",
    "    컨볼루션 적용을 위해 1칸씩 이동 필터를 이동하는것을 의미함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding : \"SAME\" 또는 \"VALID\" 값을 가짐. padding='VALID'라면\n",
    "    컨볼루션 연산 공식에 의하여 가로/세로(차원)의 크기가 축소된\n",
    "    결과가 리턴됨, 그러나 padding='SAME'으로 지정하면\n",
    "    입력값의 가로/세로(차원) 크기와 같은 출력이 리턴되도록\n",
    "    작아진 차원 부분에 0값을 채운 제로패딩을 출력함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "입력채널 : 직관적으로 데이터가 들어오는 통로라고 생각하면 무난함.\n",
    "    즉 입력채널이 1이라고 하면 데이터가 들어오는 통로가 1개이며,\n",
    "    만약 입력채널이 32개라고 하면 32개의 통로를 통해서 입력데이터가\n",
    "    들어온다고 판단하면 무리가 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.max_pooling(value,ksize,strides,padding)\n",
    "밑에 파라미터 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value : [batch, height, width, channels] / Tensor로 만들어져있는\n",
    "    형식의 입력데이터, 일반적으로 relu를 통과한 출력결과를 말한다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksize : 컨볼루션 신경망에서 일반적인 ksize는 다음과 같이\n",
    "    [1,height,width,1] 형태로 표시함. 예를 들어\n",
    "    ksize = [1,2,2,1]이라면 2칸씩 이동하면서 출력결과를 1개를\n",
    "    만들어 낸다는 것을 의미함.\n",
    "    즉 4개 (2*2) 데이터 중에서 가장 큰 값 1개를 찾아서\n",
    "    반환하는 역할을 수행함, 만약 ksize = [1,3,3,1]이라고 하면\n",
    "    3칸씩 이동, 즉 9개 (3*3) 데이터중에서 가장 큰 값을 찾는다는거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strides : max pooling 을 위해 윈도우를 이동시키는 간격을 나타냄\n",
    "    예를 들어 [1,2,2,1] 로 strides를 나타낸다면 max pooling적용을\n",
    "    위해 2칸씩 이동하는것을 의미함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding : max pooling에서의 padding값은 max pooling을 수행하기에는\n",
    "    데이터가 부족한 경우에 주변을 0등으로 채워는 역할을 함.\n",
    "    예를 들어 max pooling에서 풀링층으로 들어오는 입력데이터가\n",
    "    7*7이고, 데이터를 2개씩 묶어 최대값을 찾아내는 연산을 하기엔\n",
    "    입력으로 주어진 데이터가 부족한 상황임 (즉, 최소 8*8이여야 가능)\n",
    "    이때 padding='SAME' 이면, 부족한 데이터 부분을 0등으로 채운 후에\n",
    "    데이터를 2개씩 묶어 최대값을 뽑아낼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ICT01_22\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ICT01_22\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ICT01_22\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ICT01_22\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ICT01_22\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ICT01_22\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\ICT01_22\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ICT01_22\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ICT01_22\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ICT01_22\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ICT01_22\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ICT01_22\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-2d3ea08f8219>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_22\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_22\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_22\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_22\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_22\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "\n",
      " 55000 10000 5000\n",
      "\n",
      " train image shape =  (55000, 784)\n",
      "train label shape =  (55000, 10)\n",
      "test image shape =  (10000, 784)\n",
      "test label shape =  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "print(\"\\n\", mnist.train.num_examples, mnist.test.num_examples, mnist.validation.num_examples)\n",
    "\n",
    "print(\"\\n train image shape = \", np.shape(mnist.train.images))\n",
    "print('train label shape = ', np.shape(mnist.train.labels))\n",
    "print('test image shape = ', np.shape(mnist.test.images))\n",
    "print('test label shape = ', np.shape(mnist.test.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001 # 학습률\n",
    "epochs = 30   # 반복횟수\n",
    "batch_size = 100   # 한번에 입력으로 주어지는 MNIST 개수\n",
    "\n",
    "# 입력\n",
    "X = tf.placeholder(tf.float32,[None,784])\n",
    "T = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "# 입력층의 출력값, 컨볼루션 연산을 위해 reshape시킴\n",
    "A1 = X_img = tf.reshape(X,[-1,28,28,1]) # 이미지 28* 28 *1(black/white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1번째 컨볼루션 층\n",
    "# 3* 3 크기를 가지는 32개의 필터를 적용\n",
    "F2 = tf.Variable(tf.random_normal([3,3,1,32], stddev=0.01)) # 이게 필터\n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[32]))\n",
    "\n",
    "# 1번째 컨볼루션 연산을 통해 28 * 28* 1 -> 28*28*32\n",
    "C2 = tf.nn.conv2d(A1, F2, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "# ReLU\n",
    "Z2 = tf.nn.relu(C2+b2)\n",
    "\n",
    "# 1번째 max pooling을 통해 28*28*32 -> 14*14*32\n",
    "A2 = P2 = tf.nn.max_pool(Z2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2번째 컨볼루션 층\n",
    "# 3* 3 크기를 가지는 64개의 필터를 적용\n",
    "F3 = tf.Variable(tf.random_normal([3,3,32,64], stddev=0.01)) # 이게 필터\n",
    "b3 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "\n",
    "# 1번째 컨볼루션 연산을 통해 14 * 14* 32 -> 14*14*64\n",
    "C3 = tf.nn.conv2d(A2, F3, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "# ReLU\n",
    "Z3 = tf.nn.relu(C3+b3)\n",
    "\n",
    "# 1번째 max pooling을 통해 14*14*32 -> 7*7*64\n",
    "A3 = P3 = tf.nn.max_pool(Z3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3번째 컨볼루션 층\n",
    "# 3* 3 크기를 가지는 128개의 필터를 적용\n",
    "F4 = tf.Variable(tf.random_normal([3,3,64,128], stddev=0.01)) # 이게 필터\n",
    "b4 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "\n",
    "# 1번째 컨볼루션 연산을 통해 7 * 7* 64 -> 7*7*128\n",
    "C4 = tf.nn.conv2d(A3, F4, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "# ReLU\n",
    "Z4 = tf.nn.relu(C4+b4)\n",
    "\n",
    "# 1번째 max pooling을 통해 7*7*128 -> 4*4*128\n",
    "A4 = P4 = tf.nn.max_pool(Z4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완전 연결층\n",
    "# 4 * 4 크기를 가진 128개의 activation map을 flatten 시킴\n",
    "A4_flat = P4_flat = tf.reshape(A4, [-1, 128*4*4])\n",
    "# 2,048 개 (128*4*4) 노드를 가지도록 reshape 시켜줌,\n",
    "# 즉 완전 결정층 (2048개 노드)와 출력층 (10개노드)는 2048*10 개 노드가\n",
    "# 완전 연결되어있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력층\n",
    "W5 = tf.Variable(tf.random_normal([128*4*4, 10], stddev=0.01))\n",
    "b5 = tf.Variable(tf.random_normal([10])) # MNiST는 원핫인코딩 10개임\n",
    "\n",
    "# 출력층 선형회귀 값 Z5, 즉 softmax에 들어가는 입력값\n",
    "Z5 = logits = tf.matmul(A4_flat, W5) + b5 # 선형회귀값 Z5\n",
    "\n",
    "y = A5 = tf.nn.softmax(Z5 )\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z5, labels= T))\n",
    "# 출력층 선형회귀 값(logits) Z5 와 정답 T를 이용하여\n",
    "# 손실함수 크로스 엔트로피(cross_entropy) 계산\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate) \n",
    "# 성능개선을 위해 AdamOptimizer를 사용\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size X 10 데이터에 대해 argmax를 통해 행단위로 비교함\n",
    "predicted_val = tf.equal(tf.argmax(A5,1), tf.argmax(T,1))\n",
    "# 출력층의 계산 값 A5와 정답 T에 대해 , 행 단위로 값을 비교함\n",
    "\n",
    "\n",
    "# batch_size X 10 의 True, False 를 1또는 0으로 변환\n",
    "accuracy = tf.reduce_mean(tf.cast(predicted_val, dtype=tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-6e96a072f883>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-6e96a072f883>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    출력층 선형회귀 값(logits) Z5 와 정답 T를 이용하여 손실함수 loss 정의,\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "출력층 선형회귀 값(logits) Z5 와 정답 T를 이용하여 손실함수 loss 정의,\n",
    "여기서 한번에 입력되는 배치 사이즈가 100이므로 Z5 와 T shape는 (100*10)이며,\n",
    "tf.nn.softmax_cross_entropy_with_logits_v2(...) 에 의해\n",
    "100개의 데이터에 대해 각각의 소프트 맥스가 계산된 후에 정답과이 비교를 통해\n",
    "크로스 엔트로피 손실 함수 값이 계산되고, tf.reduce_mean(...)에 의해서 100개의\n",
    "손실함수 값의 평균이 계산됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-77e3a7da22a1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-77e3a7da22a1>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    one-hot encoding에 의해서 출력 층 계산 값 A5와 정답\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "one-hot encoding에 의해서 출력 층 계산 값 A5와 정답\n",
    "T는 (bacth_size X 10) shape을 가지는 행렬임. 따라서\n",
    "argmax의 두번쨰 인자에 1을 주어 행 단위로 A5와 T를 비교함.\n",
    "위 예에서는 batch_size가 100이므로 ( 100*10 )\n",
    "행렬에 대해서 행 단위로 비교하고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  0 , step =  0 , loss_val =  2.8773477\n",
      "epochs =  0 , step =  100 , loss_val =  2.2867455\n",
      "epochs =  0 , step =  200 , loss_val =  0.41998982\n",
      "epochs =  0 , step =  300 , loss_val =  0.17003334\n",
      "epochs =  0 , step =  400 , loss_val =  0.09060123\n",
      "epochs =  0 , step =  500 , loss_val =  0.21094361\n",
      "epochs =  1 , step =  0 , loss_val =  0.0820261\n",
      "epochs =  1 , step =  100 , loss_val =  0.032795984\n",
      "epochs =  1 , step =  200 , loss_val =  0.08268248\n",
      "epochs =  1 , step =  300 , loss_val =  0.10964556\n",
      "epochs =  1 , step =  400 , loss_val =  0.28803194\n",
      "epochs =  1 , step =  500 , loss_val =  0.020062637\n",
      "epochs =  2 , step =  0 , loss_val =  0.09322067\n",
      "epochs =  2 , step =  100 , loss_val =  0.033475053\n",
      "epochs =  2 , step =  200 , loss_val =  0.036223736\n",
      "epochs =  2 , step =  300 , loss_val =  0.037242718\n",
      "epochs =  2 , step =  400 , loss_val =  0.019129358\n",
      "epochs =  2 , step =  500 , loss_val =  0.042302083\n",
      "epochs =  3 , step =  0 , loss_val =  0.046007276\n",
      "epochs =  3 , step =  100 , loss_val =  0.030174201\n",
      "epochs =  3 , step =  200 , loss_val =  0.009115946\n",
      "epochs =  3 , step =  300 , loss_val =  0.03022109\n",
      "epochs =  3 , step =  400 , loss_val =  0.028208053\n",
      "epochs =  3 , step =  500 , loss_val =  0.01552999\n",
      "epochs =  4 , step =  0 , loss_val =  0.011693252\n",
      "epochs =  4 , step =  100 , loss_val =  0.1261545\n",
      "epochs =  4 , step =  200 , loss_val =  0.014575023\n",
      "epochs =  4 , step =  300 , loss_val =  0.0059299893\n",
      "epochs =  4 , step =  400 , loss_val =  0.06857025\n",
      "epochs =  4 , step =  500 , loss_val =  0.013447818\n",
      "epochs =  5 , step =  0 , loss_val =  0.006105661\n",
      "epochs =  5 , step =  100 , loss_val =  0.046628702\n",
      "epochs =  5 , step =  200 , loss_val =  0.010610976\n",
      "epochs =  5 , step =  300 , loss_val =  0.0205364\n",
      "epochs =  5 , step =  400 , loss_val =  0.013712075\n",
      "epochs =  5 , step =  500 , loss_val =  0.001016962\n",
      "epochs =  6 , step =  0 , loss_val =  0.01617959\n",
      "epochs =  6 , step =  100 , loss_val =  0.023635827\n",
      "epochs =  6 , step =  200 , loss_val =  0.005386218\n",
      "epochs =  6 , step =  300 , loss_val =  0.0031681634\n",
      "epochs =  6 , step =  400 , loss_val =  0.07899574\n",
      "epochs =  6 , step =  500 , loss_val =  0.036114536\n",
      "epochs =  7 , step =  0 , loss_val =  0.0010589433\n",
      "epochs =  7 , step =  100 , loss_val =  0.0032875068\n",
      "epochs =  7 , step =  200 , loss_val =  0.0009310459\n",
      "epochs =  7 , step =  300 , loss_val =  0.0035860694\n",
      "epochs =  7 , step =  400 , loss_val =  0.013672509\n",
      "epochs =  7 , step =  500 , loss_val =  0.0018639568\n",
      "epochs =  8 , step =  0 , loss_val =  0.03204327\n",
      "epochs =  8 , step =  100 , loss_val =  0.019259872\n",
      "epochs =  8 , step =  200 , loss_val =  0.022729088\n",
      "epochs =  8 , step =  300 , loss_val =  0.026422959\n",
      "epochs =  8 , step =  400 , loss_val =  0.00092181785\n",
      "epochs =  8 , step =  500 , loss_val =  0.022629356\n",
      "epochs =  9 , step =  0 , loss_val =  0.044383615\n",
      "epochs =  9 , step =  100 , loss_val =  0.0037137123\n",
      "epochs =  9 , step =  200 , loss_val =  0.004206166\n",
      "epochs =  9 , step =  300 , loss_val =  0.0025776743\n",
      "epochs =  9 , step =  400 , loss_val =  0.001892293\n",
      "epochs =  9 , step =  500 , loss_val =  0.022070944\n",
      "epochs =  10 , step =  0 , loss_val =  7.4054085e-05\n",
      "epochs =  10 , step =  100 , loss_val =  0.0037729645\n",
      "epochs =  10 , step =  200 , loss_val =  0.008252203\n",
      "epochs =  10 , step =  300 , loss_val =  0.00035305516\n",
      "epochs =  10 , step =  400 , loss_val =  0.00034561276\n",
      "epochs =  10 , step =  500 , loss_val =  0.00089296815\n",
      "epochs =  11 , step =  0 , loss_val =  0.006873719\n",
      "epochs =  11 , step =  100 , loss_val =  0.0004610378\n",
      "epochs =  11 , step =  200 , loss_val =  0.014703047\n",
      "epochs =  11 , step =  300 , loss_val =  0.00034841642\n",
      "epochs =  11 , step =  400 , loss_val =  0.024112836\n",
      "epochs =  11 , step =  500 , loss_val =  0.0030995107\n",
      "epochs =  12 , step =  0 , loss_val =  0.0035212224\n",
      "epochs =  12 , step =  100 , loss_val =  0.0009650269\n",
      "epochs =  12 , step =  200 , loss_val =  0.0035845002\n",
      "epochs =  12 , step =  300 , loss_val =  0.027316647\n",
      "epochs =  12 , step =  400 , loss_val =  9.182487e-05\n",
      "epochs =  12 , step =  500 , loss_val =  0.036792077\n",
      "epochs =  13 , step =  0 , loss_val =  0.0063209776\n",
      "epochs =  13 , step =  100 , loss_val =  0.0008163747\n",
      "epochs =  13 , step =  200 , loss_val =  0.0016843766\n",
      "epochs =  13 , step =  300 , loss_val =  0.0002453912\n",
      "epochs =  13 , step =  400 , loss_val =  0.0005812397\n",
      "epochs =  13 , step =  500 , loss_val =  0.0028587598\n",
      "epochs =  14 , step =  0 , loss_val =  0.00043605446\n",
      "epochs =  14 , step =  100 , loss_val =  0.032214295\n",
      "epochs =  14 , step =  200 , loss_val =  0.0006897369\n",
      "epochs =  14 , step =  300 , loss_val =  0.0029226928\n",
      "epochs =  14 , step =  400 , loss_val =  0.0066564004\n",
      "epochs =  14 , step =  500 , loss_val =  0.007287578\n",
      "epochs =  15 , step =  0 , loss_val =  0.001985541\n",
      "epochs =  15 , step =  100 , loss_val =  0.00020726054\n",
      "epochs =  15 , step =  200 , loss_val =  4.6422283e-05\n",
      "epochs =  15 , step =  300 , loss_val =  3.92504e-05\n",
      "epochs =  15 , step =  400 , loss_val =  7.9812446e-05\n",
      "epochs =  15 , step =  500 , loss_val =  0.0349925\n",
      "epochs =  16 , step =  0 , loss_val =  0.0003149029\n",
      "epochs =  16 , step =  100 , loss_val =  0.038812146\n",
      "epochs =  16 , step =  200 , loss_val =  0.009126755\n",
      "epochs =  16 , step =  300 , loss_val =  0.002012411\n",
      "epochs =  16 , step =  400 , loss_val =  0.00860664\n",
      "epochs =  16 , step =  500 , loss_val =  0.00035040735\n",
      "epochs =  17 , step =  0 , loss_val =  0.00030370872\n",
      "epochs =  17 , step =  100 , loss_val =  0.00079745136\n",
      "epochs =  17 , step =  200 , loss_val =  0.013815327\n",
      "epochs =  17 , step =  300 , loss_val =  0.0038854068\n",
      "epochs =  17 , step =  400 , loss_val =  0.048888996\n",
      "epochs =  17 , step =  500 , loss_val =  0.006704415\n",
      "epochs =  18 , step =  0 , loss_val =  0.032248966\n",
      "epochs =  18 , step =  100 , loss_val =  0.0004981835\n",
      "epochs =  18 , step =  200 , loss_val =  0.00078645826\n",
      "epochs =  18 , step =  300 , loss_val =  0.00016669722\n",
      "epochs =  18 , step =  400 , loss_val =  0.0006838268\n",
      "epochs =  18 , step =  500 , loss_val =  2.0136533e-05\n",
      "epochs =  19 , step =  0 , loss_val =  8.281731e-06\n",
      "epochs =  19 , step =  100 , loss_val =  1.6086116e-05\n",
      "epochs =  19 , step =  200 , loss_val =  0.0018819713\n",
      "epochs =  19 , step =  300 , loss_val =  0.002617366\n",
      "epochs =  19 , step =  400 , loss_val =  5.5512628e-05\n",
      "epochs =  19 , step =  500 , loss_val =  2.2474682e-05\n",
      "epochs =  20 , step =  0 , loss_val =  0.00046653324\n",
      "epochs =  20 , step =  100 , loss_val =  0.0001424445\n",
      "epochs =  20 , step =  200 , loss_val =  0.00038096833\n",
      "epochs =  20 , step =  300 , loss_val =  0.0022643632\n",
      "epochs =  20 , step =  400 , loss_val =  2.4481946e-05\n",
      "epochs =  20 , step =  500 , loss_val =  0.013915744\n",
      "epochs =  21 , step =  0 , loss_val =  3.5544246e-05\n",
      "epochs =  21 , step =  100 , loss_val =  0.029687906\n",
      "epochs =  21 , step =  200 , loss_val =  2.985936e-05\n",
      "epochs =  21 , step =  300 , loss_val =  0.0004004589\n",
      "epochs =  21 , step =  400 , loss_val =  0.00018390971\n",
      "epochs =  21 , step =  500 , loss_val =  0.007814767\n",
      "epochs =  22 , step =  0 , loss_val =  0.006724723\n",
      "epochs =  22 , step =  100 , loss_val =  0.0052617346\n",
      "epochs =  22 , step =  200 , loss_val =  0.0030324743\n",
      "epochs =  22 , step =  300 , loss_val =  0.00031325896\n",
      "epochs =  22 , step =  400 , loss_val =  2.1786564e-05\n",
      "epochs =  22 , step =  500 , loss_val =  6.746841e-06\n",
      "epochs =  23 , step =  0 , loss_val =  7.829662e-06\n",
      "epochs =  23 , step =  100 , loss_val =  0.013713143\n",
      "epochs =  23 , step =  200 , loss_val =  0.00048067802\n",
      "epochs =  23 , step =  300 , loss_val =  3.3352987e-06\n",
      "epochs =  23 , step =  400 , loss_val =  3.649765e-06\n",
      "epochs =  23 , step =  500 , loss_val =  4.0960065e-05\n",
      "epochs =  24 , step =  0 , loss_val =  0.0011813301\n",
      "epochs =  24 , step =  100 , loss_val =  0.0032098652\n",
      "epochs =  24 , step =  200 , loss_val =  0.0010816315\n",
      "epochs =  24 , step =  300 , loss_val =  0.00023440555\n",
      "epochs =  24 , step =  400 , loss_val =  0.00073933305\n",
      "epochs =  24 , step =  500 , loss_val =  0.0002433023\n",
      "epochs =  25 , step =  0 , loss_val =  2.873779e-05\n",
      "epochs =  25 , step =  100 , loss_val =  7.7944605e-06\n",
      "epochs =  25 , step =  200 , loss_val =  0.00078410655\n",
      "epochs =  25 , step =  300 , loss_val =  0.00022561323\n",
      "epochs =  25 , step =  400 , loss_val =  0.00033957497\n",
      "epochs =  25 , step =  500 , loss_val =  0.0022049404\n",
      "epochs =  26 , step =  0 , loss_val =  1.23025975e-05\n",
      "epochs =  26 , step =  100 , loss_val =  0.00013801813\n",
      "epochs =  26 , step =  200 , loss_val =  2.4248962e-05\n",
      "epochs =  26 , step =  300 , loss_val =  0.0026564244\n",
      "epochs =  26 , step =  400 , loss_val =  5.8861388e-05\n",
      "epochs =  26 , step =  500 , loss_val =  0.0021155176\n",
      "epochs =  27 , step =  0 , loss_val =  0.0008848094\n",
      "epochs =  27 , step =  100 , loss_val =  0.003095166\n",
      "epochs =  27 , step =  200 , loss_val =  0.0006019473\n",
      "epochs =  27 , step =  300 , loss_val =  0.0003909353\n",
      "epochs =  27 , step =  400 , loss_val =  0.00013727283\n",
      "epochs =  27 , step =  500 , loss_val =  0.00035475346\n",
      "epochs =  28 , step =  0 , loss_val =  2.0035766e-05\n",
      "epochs =  28 , step =  100 , loss_val =  4.803676e-06\n",
      "epochs =  28 , step =  200 , loss_val =  0.0040447675\n",
      "epochs =  28 , step =  300 , loss_val =  7.903954e-05\n",
      "epochs =  28 , step =  400 , loss_val =  0.00094721955\n",
      "epochs =  28 , step =  500 , loss_val =  0.0033679719\n",
      "epochs =  29 , step =  0 , loss_val =  3.2442826e-05\n",
      "epochs =  29 , step =  100 , loss_val =  2.2046257e-05\n",
      "epochs =  29 , step =  200 , loss_val =  0.002986269\n",
      "epochs =  29 , step =  300 , loss_val =  0.009787488\n",
      "epochs =  29 , step =  400 , loss_val =  0.0022887834\n",
      "epochs =  29 , step =  500 , loss_val =  0.000125676\n",
      "\n",
      "elapsed time =  0:24:15.957361\n",
      "\n",
      "Accuracy =  0.9905\n"
     ]
    }
   ],
   "source": [
    "with  tf.Session()  as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())  # 변수 노드(tf.Variable) 초기화\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for i in range(epochs):    # 30 번 반복수행\n",
    "        \n",
    "        total_batch = int(mnist.train.num_examples / batch_size)  # 55,000 / 100\n",
    "\n",
    "        for step in range(total_batch):\n",
    "            \n",
    "            batch_x_data, batch_t_data = mnist.train.next_batch(batch_size)\n",
    "      \n",
    "            loss_val, _ = sess.run([loss, train], feed_dict={X: batch_x_data, T: batch_t_data})    \n",
    "        \n",
    "            if step % 100 == 0:\n",
    "                print(\"epochs = \", i, \", step = \", step, \", loss_val = \", loss_val)\n",
    "                save_path = saver.save(sess, \"/tmp/mnist_model_set.ckpt\")\n",
    "    \n",
    "    end_time = datetime.now() \n",
    "    \n",
    "    print(\"\\nelapsed time = \", end_time - start_time) \n",
    "    \n",
    "    # Accuracy 확인\n",
    "    test_x_data = mnist.test.images    # 10000 X 784\n",
    "    test_t_data = mnist.test.labels    # 10000 X 10\n",
    "    \n",
    "    accuracy_val = sess.run(accuracy, feed_dict={X: test_x_data, T: test_t_data})\n",
    "    \n",
    "    print(\"\\nAccuracy = \", accuracy_val)\n",
    "    save_path = saver.save(sess, \"/tmp/mnist_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"/tmp/mnist_model_final.ckpt.meta\")\n",
    "with  tf.Session()  as sess:\n",
    "    saver.restore(sess, \"/tmp/mnist_model_final.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
